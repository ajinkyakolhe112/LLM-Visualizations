<!DOCTYPE html>
<html>
<head>
    <title>LLM Tokenizer</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" type="image/svg+xml" href="favicons/token.svg" />
    <link href="tokenizer.css" rel="stylesheet">
</head>
<body>
    <h1>
        <img src="favicons/token.svg" alt="Token">
        <img src="favicons/token.svg" alt="Token">
        <img src="favicons/token.svg" alt="Token">
        <span>Online LLM Tokenizer</span>
        <img src="favicons/token.svg" alt="Token">
        <img src="favicons/token.svg" alt="Token">
        <img src="favicons/token.svg" alt="Token">
    </h1>
    <p>
        A pure Javascript tokenizer running in your browser that can load <code>tokenizer.json</code> and
        <code>tokenizer_config.json</code> from any repository on Huggingface. You can use it to count tokens and
        compare how different large language model vocabularies work. It's also useful for debugging prompt templates.
        If you are wondering why are there so many models under Xenova, it's because they work for HuggingFace and
        re-upload just the tokenizers, so it's possible to load them without agreeing to model licences.
    </p>
    <textarea id="textInput" name="textInput" autofocus placeholder="Enter the text you want to tokenize"
        style="height: 5em;">
[INST] <<SYS>>
You are a friendly Llama.
<</SYS>>

Do you spit at people?[/INST]</textarea>
    <ul id="models"></ul>
    <div id="addModel">
        <input placeholder="01-ai/Yi-34B" />
        <button>Add tokenizer from HuggingFace</button>
    </div>

    <script type="module">
        import { AutoTokenizer } from "./transformers.js";

        const KEY_MODELS = "models";
        const COLOURS = [
            "E40303", "FF8C00", "FFED00", "008026", "061393",
            "732982", "5BCEFA", "F5A9B8", "8F3F2B", "FFFFFF"
        ];

        let models = [];
        const loadedModels = {};
        const modelsList = document.getElementById("models");
        const textInput = document.getElementById("textInput");
        const addModelBox = document.getElementById("addModel");

        // Need to add 2 pixels to account for the borders
        textInput.setAttribute("style", `height:${textInput.scrollHeight + 2}px;`);
        let textInputContent = textInput.value;

        function loadModels() {
            const storedModels = localStorage.getItem(KEY_MODELS);
            try {
                if (storedModels === null)
                    throw Error("No models found in LocalStorage, using default list.");
                models = JSON.parse(storedModels);
            } catch (error) {
                console.log(error);
                models = [
                    "Xenova/gpt-4",
                    "Xenova/gpt-3",
                    "Xenova/llama-3-tokenizer",
                    "hf-internal-testing/llama-tokenizer",
                    "Xenova/gemma-tokenizer",
                    "microsoft/Phi-3-mini-4k-instruct",
                    "mistral-community/Mixtral-8x22B-v0.1"
                ];
                saveModels();
            }
        }

        function saveModels() {
            localStorage.setItem(KEY_MODELS, JSON.stringify(models));
        }

        function addModel(name) {
            localStorage.setItem(KEY_MODELS, JSON.stringify([...models, name]));
        }

        const renderTokenAndText = (acc, { token, text }, index) => {
            return (acc +=
                text === "\n"
                    ? "<br>"
                    : `<span class="token">${token}</span><code style="background: #${
                        COLOURS[index % COLOURS.length]
                    }66">${text
                        .replace(/&/g, "&amp;")
                        .replace(/</g, "&lt;")
                        .replace(/>/g, "&gt;")}</code>`);
        };

        async function loadTokenizers() {
            console.log("Loading models...");
            for (const model of models) {
                if (!(model in loadedModels)) {
                    try {
                        console.log("Loading model: ", model);
                        loadedModels[model] = await AutoTokenizer.from_pretrained(model);
                    } catch (error) {
                        console.error("Model loading error:", error);
                        loadedModels[model] = { error };
                    }

                    console.log("Loaded model", loadedModels[model]);
                    // some tokenizers strip spaces, let's prevent it so we can render them with the token numbers
                    if (
                        loadedModels[model]?.decoder?.decoders?.at(-1)?.config?.type === "Strip"
                    ) {
                        loadedModels[model].decoder.decoders.pop();
                    }

                    const newModelListItem = document.createElement("li");
                    newModelListItem.dataset.model = model;
                    modelsList.appendChild(newModelListItem);
                }
            }
            updateTokens();
        }

        function updateTokens() {
            for (const [modelName, model] of Object.entries(loadedModels)) {
                let modelBlockWithTextAndTokens = "";
                if (model.error) {
                    modelBlockWithTextAndTokens = `
                        <h2>${modelName}</h2>
                        <p style='white-space: pre-line; color: red;'>
                            Model doesn't exist on HuggingFace, doesn't have the required JSON files or needs licence agreement. Original error message:\n${model.error}
                        </p>`;
                } else {
                    const tokens = model.encode(textInputContent);
                    const textFromTokens = model
                        .batch_decode(
                            tokens.map((token) => [token]),
                            { clean_up_tokenization_spaces: false },
                        )
                        .map((text, index) => ({ text, token: tokens[index] }))
                        .reduce(renderTokenAndText, "");

                    modelBlockWithTextAndTokens = `
                        <h2>${modelName} <img src="favicons/token.svg" alt="Token"> Token count: ${tokens.length}</h2>
                        ${textFromTokens}
                    `;
                }
                document.querySelector(`li[data-model="${modelName}"]`).innerHTML =
                    modelBlockWithTextAndTokens;
            }
        }

        textInput.addEventListener("input", (event) => {
            textInput.style.height = 0;
            textInput.style.height = `${textInput.scrollHeight + 2}px`;
            textInputContent = event.target.value;
            updateTokens();
        });

        addModelBox.querySelector("button").addEventListener("click", async () => {
            addModel(addModelBox.querySelector("input").value);
            loadModels();
            await loadTokenizers();
            window.scrollTo(0, document.body.scrollHeight);
        });

        loadModels();
        await loadTokenizers();
    </script>
</body>
</html> 